---
layout: post
title: "Implementing Q-learning"
categories: reinforcement-learning
author: Coder-RG
---

This is the first article under Reinforcement Learning. Today, we will look at
[Slider], a simple example for understanding [Q-learning](https://en.wikipedia.org/wiki/Q-learning).
Hopefully, you will have a clear understanding after reading this article.

## Environment

{:refdef: style="text-align: center;"}
![Slider_env]({{site.baseurl}}/assets/images/env1.png)
{: refdef}

Our environment is a 1x7 grid. **S** is the Start state and **G** is the goal state.
The agent can only move left or right. When the agent arrives at **G** the game ends.

{:refdef: style="text-align: center;"}
![Slider_env]({{site.baseurl}}/assets/images/env_reward.png)
{: refdef}

*Rewards*
- +1 for moving towards *G*
- -10 on illegal step, i.e moving out of the grid
- +10 on reaching *G*


## Coding
```python
def run_exp(num_iterations=100):
    game = Slider()
    reward_graph = []
    episode_graph = []

    game.reset()
    for episode in range(num_iterations):
        state = 0
        # Used to count the number of moves in a single iteration
        moves = 0
        episodic_reward = 0
        while state != 6:# and moves < 20:
            action = game.get_action(state)
            new_state, reward = game.step(state, action)
            # Temporal Difference
            action = 0 if action == -1 else 1
            temp_diff = reward + max(game.q_table[new_state]) - game.q_table[state][action]
            # Q_learning
            game.q_table[state][action] += game.alpha * temp_diff
            state = new_state
            moves += 1
            episodic_reward += reward
        episode_graph.append(moves)
        reward_graph.append(episodic_reward)
        game.reset()
```

## Result

### Q-table

|State|<-|->|
|:---:|:----:|:----:|
|0|-0.98|1.13|
|1|0.05|1.25|
|2|0.12|1.39|
|3|0.08|1.95|
|4|0.06|3.55|
|5|0.18|6.34|
|6|0.00|0.00|

### Policy

### Graphs

![slider_graph]({{site.baseurl}}/assets/images/slider_graph.png)