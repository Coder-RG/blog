<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Q-Learning with epsilon-decay | Techcopia</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Q-Learning with epsilon-decay" />
<meta name="author" content="Rishabh Goel" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="In this article, we will implement Q-learning with epsilon-decay. We implemented Q-learning in the previous article and therefore won’t be going into any detail. Come back after reading this article if you haven’t already." />
<meta property="og:description" content="In this article, we will implement Q-learning with epsilon-decay. We implemented Q-learning in the previous article and therefore won’t be going into any detail. Come back after reading this article if you haven’t already." />
<link rel="canonical" href="http://localhost:4000/blog/reinforcement-learning/slider-with-epsilon-decay.html" />
<meta property="og:url" content="http://localhost:4000/blog/reinforcement-learning/slider-with-epsilon-decay.html" />
<meta property="og:site_name" content="Techcopia" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-01-01T00:00:00+05:30" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Q-Learning with epsilon-decay" />
<script type="application/ld+json">
{"@type":"BlogPosting","url":"http://localhost:4000/blog/reinforcement-learning/slider-with-epsilon-decay.html","description":"In this article, we will implement Q-learning with epsilon-decay. We implemented Q-learning in the previous article and therefore won’t be going into any detail. Come back after reading this article if you haven’t already.","headline":"Q-Learning with epsilon-decay","dateModified":"2022-01-01T00:00:00+05:30","datePublished":"2022-01-01T00:00:00+05:30","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/blog/reinforcement-learning/slider-with-epsilon-decay.html"},"author":{"@type":"Person","name":"Rishabh Goel"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/main.css">
  <link rel="shortcut icon" type="image/ico" href="/blog/favicon.ico">
  <link rel="icon" type="image/png" sizes="32x32" href="/blog/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/blog/favicon-16x16.png"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/blog/feed.xml" title="Techcopia" /><meta name="google-site-verification" content="ed7E7DKesZM2qv5PWcE9XcCBAThpRjK6I0Min5AzK-c" />
</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">Techcopia</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Q-Learning with epsilon-decay</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2022-01-01T00:00:00+05:30" itemprop="datePublished">Published: Jan 1, 2022 •</time>• <span itemprop="author" itemscope itemtype="http://schema.org/Person"><span class="p-author h-card" itemprop="name">Rishabh Goel</span></span></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>In this article, we will implement Q-learning with epsilon-decay. We implemented
Q-learning in the previous article and therefore won’t be going into any detail.
Come back after reading <a href="/blog//reinforcement-learning/slider.html">this</a> article if you haven’t already.</p>

<p><strong>what is epsilon-greedy?</strong></p>

<p>It is an algorithm in reinforcement learning
that controls the agent’s <a href="https://www.baeldung.com/cs/epsilon-greedy-q-learning#1-exploration-vs-exploitation-tradeoff">exploration v/s eploitation tradeoff</a>.</p>

<p><strong>Why is this a tradeoff?</strong></p>

<p>Because eploration helps the agent find the best possible actions, with a low episodic
reward being the possibility. Eploitation on the other hand refers to taking
actions that maximise the short-term reward. This becomes an issue when the agent
has not explored its environment and takes actions based on its current knowledge
when it is possible that a higher reward action might be unexplored.</p>

<p>This is why we use epsilon-greedy algorithm. However, as the agent learns its
environment, there is no need to to explore anymore. This can be issue with static epsilon
value. Therefore, epsilon-decay is implemented to make it dynamic.</p>

<h2 id="environment">Environment</h2>

<p style="text-align: center;"><img src="/blog/assets/images/env1.png" alt="Slider_env" /></p>

<p><strong>S</strong> is the start state and <strong>G</strong> is the goal state. Invalid moves are as follows:</p>
<ul>
  <li>moving left when on <strong>S</strong></li>
  <li>moving right when on <strong>G</strong></li>
</ul>

<p>Currently, the episode ends on <strong>G</strong>, therefore the agent is not able to take any action
on reaching <strong>G</strong>.</p>

<h2 id="implementation">Implementation</h2>

<p>The value of epsilon is changed only when the agent reaches the goal state.
This is how it is implemented for this example. You can check out complete code on
<a href="https://github.com/Coder-RG/blog-content/tree/master/slider">github</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="p">...</span>
    <span class="k">if</span> <span class="n">new_state</span> <span class="o">==</span> <span class="mi">6</span><span class="p">:</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="mi">10</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">epsilon</span> <span class="o">*=</span> <span class="bp">self</span><span class="p">.</span><span class="n">decay_rate</span>
    <span class="p">...</span>
</code></pre></div></div>
<p>The <code class="language-plaintext highlighter-rouge">decay_rate</code> is set to <code class="language-plaintext highlighter-rouge">0.99</code> in all the examples. You can experiment will various
different values like <code class="language-plaintext highlighter-rouge">0.9</code> or <code class="language-plaintext highlighter-rouge">0.999</code> or something different altogether, maybe?</p>

<h2 id="performance">Performance</h2>

<p>The reward system can lead to very different behaviour of our agent. Keeping this
in mind, here are three different reward systems with their performance charts.</p>

<p>In our first reward system, the agent received <code class="language-plaintext highlighter-rouge">+1</code> reward for moving towards the goal state,
<code class="language-plaintext highlighter-rouge">-10</code> for invalid move and <code class="language-plaintext highlighter-rouge">+10</code> for reaching the goal state. The episodes only end when
the agent reaches the goal state, which is evident from the chart below. Our agent took its
merry time to reach the goal state, receiving huge amount of reward in the process.</p>

<p><img src="/blog/assets/images/SliderEpsilonDecay0.png" alt="Epsilon-Greedy" /></p>

<p>In our second reward system, the agent received <code class="language-plaintext highlighter-rouge">-10</code> for invalid move and <code class="language-plaintext highlighter-rouge">+10</code> for reaching
the goal state. However, this time there is no reward for any other move. Since there was no reward
other than for reaching goal state, the max reward is 10 in the chart below. Something else to note
is that the spikes in episode lengths decrease over time. The same is true with the next example too.</p>

<p><img src="/blog/assets/images/SliderEpsilonDecay1.png" alt="Epsilon-Greedy" /></p>

<p>Finally, in our third reward system, the agent received <code class="language-plaintext highlighter-rouge">-10</code> for invalid move and <code class="language-plaintext highlighter-rouge">+10</code> for
reaching the goal state. Moreover, the agent is penalised for all other actions, i.e. <code class="language-plaintext highlighter-rouge">-1</code> reward.
This reward system will penalise the agent for long episode lengths.</p>

<p><img src="/blog/assets/images/SliderEpsilonDecay2.png" alt="Epsilon-Greedy" /></p>

<p>A final observation is that the episode length are comparatively small in second
and third reward system. Only in the first example does the episode length grow over time.</p>


  </div><a class="u-url" href="/blog/reinforcement-learning/slider-with-epsilon-decay.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Techcopia</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Techcopia</li><li><a class="u-email" href="mailto:coder_g@pm.me">coder_g@pm.me</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/Coder-RG"><svg class="svg-icon"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg> <span class="username">Coder-RG</span></a></li><li><a href="https://www.linkedin.com/in/r1shabhg0el"><svg class="svg-icon"><use xlink:href="/blog/assets/minima-social-icons.svg#linkedin"></use></svg> <span class="username">r1shabhg0el</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>A blog containing articles on reinforcement learning.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
